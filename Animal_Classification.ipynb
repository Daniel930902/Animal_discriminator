{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "  \n",
    "# load the VGG16 network *pre-trained* on the ImageNet dataset\n",
    "\n",
    "base_model = keras.applications.VGG16(weights=\"imagenet\",\n",
    "                   input_shape=(224, 224, 3),\n",
    "                  include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Separately from setting trainable on the model, we set training to False \n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# A Dense classifier with 8 unites (categorical)\n",
    "outputs = keras.layers.Dense(8)(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 14,718,792\n",
      "Trainable params: 4,104\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to use categorical crossentropy and categorical accuracy as we now have a categorical classification problem\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits = True), metrics=[keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/train/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4a9b43cf252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/train/.ipynb_checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/valid/.ipynb_checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Find the hidden file\n",
    "\n",
    "os.listdir(\"dataset/train/\")\n",
    "os.listdir(\"dataset/valid/\")\n",
    "\n",
    "# Remove it\n",
    "\n",
    "shutil.rmtree(\"dataset/train/.ipynb_checkpoints\")\n",
    "shutil.rmtree(\"dataset/valid/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create a data generator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale = 1./224 ,\n",
    "        samplewise_center=True,  # set each sample mean to 0\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False) # we don't expect Bo to be upside-down so we will not flip vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9833 images belonging to 8 classes.\n",
      "Found 4195 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "\n",
    "train_ds = datagen.flow_from_directory('dataset/train/', \n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode='categorical',\n",
    "                                       batch_size=32)\n",
    "# load and iterate validation dataset\n",
    "\n",
    "valid_ds = datagen.flow_from_directory('dataset/valid/', \n",
    "                                      target_size=(224, 224), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "12/12 [==============================] - 12s 1s/step - loss: 2.1075 - categorical_accuracy: 0.1458 - val_loss: 2.0598 - val_categorical_accuracy: 0.1797\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 7s 569ms/step - loss: 2.0000 - categorical_accuracy: 0.2083 - val_loss: 1.9793 - val_categorical_accuracy: 0.2305\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 7s 565ms/step - loss: 1.9725 - categorical_accuracy: 0.2917 - val_loss: 1.8929 - val_categorical_accuracy: 0.4570\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 1.9434 - categorical_accuracy: 0.3359 - val_loss: 1.9049 - val_categorical_accuracy: 0.3438\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 7s 568ms/step - loss: 1.8973 - categorical_accuracy: 0.3750 - val_loss: 1.8833 - val_categorical_accuracy: 0.3281\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 7s 544ms/step - loss: 1.8670 - categorical_accuracy: 0.3568 - val_loss: 1.8487 - val_categorical_accuracy: 0.4023\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 7s 548ms/step - loss: 1.8208 - categorical_accuracy: 0.4557 - val_loss: 1.7931 - val_categorical_accuracy: 0.4219\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 7s 557ms/step - loss: 1.8188 - categorical_accuracy: 0.4557 - val_loss: 1.7660 - val_categorical_accuracy: 0.5508\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 7s 566ms/step - loss: 1.7628 - categorical_accuracy: 0.4401 - val_loss: 1.7587 - val_categorical_accuracy: 0.4531\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 7s 562ms/step - loss: 1.7273 - categorical_accuracy: 0.4792 - val_loss: 1.7042 - val_categorical_accuracy: 0.4844\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 1.7082 - categorical_accuracy: 0.4661 - val_loss: 1.6605 - val_categorical_accuracy: 0.5508\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 7s 566ms/step - loss: 1.6964 - categorical_accuracy: 0.5260 - val_loss: 1.6627 - val_categorical_accuracy: 0.5742\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 1.6668 - categorical_accuracy: 0.5443 - val_loss: 1.6342 - val_categorical_accuracy: 0.5586\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 1.6307 - categorical_accuracy: 0.5339 - val_loss: 1.6050 - val_categorical_accuracy: 0.5312\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 1.6297 - categorical_accuracy: 0.5312 - val_loss: 1.6692 - val_categorical_accuracy: 0.4846\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 7s 545ms/step - loss: 1.6218 - categorical_accuracy: 0.5260 - val_loss: 1.5567 - val_categorical_accuracy: 0.6016\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 1.5721 - categorical_accuracy: 0.5859 - val_loss: 1.5672 - val_categorical_accuracy: 0.5586\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 1.5613 - categorical_accuracy: 0.5781 - val_loss: 1.4940 - val_categorical_accuracy: 0.6289\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 1.5548 - categorical_accuracy: 0.5573 - val_loss: 1.5701 - val_categorical_accuracy: 0.5156\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 7s 600ms/step - loss: 1.5290 - categorical_accuracy: 0.5703 - val_loss: 1.4932 - val_categorical_accuracy: 0.5781\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 7s 601ms/step - loss: 1.5174 - categorical_accuracy: 0.5339 - val_loss: 1.4676 - val_categorical_accuracy: 0.6211\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 7s 562ms/step - loss: 1.5235 - categorical_accuracy: 0.5234 - val_loss: 1.5140 - val_categorical_accuracy: 0.5859\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 7s 613ms/step - loss: 1.4986 - categorical_accuracy: 0.5625 - val_loss: 1.4608 - val_categorical_accuracy: 0.6016\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 1.4644 - categorical_accuracy: 0.5807 - val_loss: 1.4018 - val_categorical_accuracy: 0.6602\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 7s 556ms/step - loss: 1.4630 - categorical_accuracy: 0.6276 - val_loss: 1.3836 - val_categorical_accuracy: 0.6250\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4343 - categorical_accuracy: 0.6011"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(train_ds , steps_per_epoch=12 , batch_size = 32 , validation_data = valid_ds , validation_steps=8 , epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "# Re-Compile model\n",
    "\n",
    "model.compile( optimizer=keras.optimizers.SGD(learning_rate = 0.002),  # learning rate\n",
    "              loss=keras.losses.CategoricalCrossentropy(from_logits = True),\n",
    "              metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "# Train again\n",
    "\n",
    "# Vaiualize accuracy/loss history\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, steps_per_epoch = 10 , batch_size = 32 , validation_data=valid_ds, validation_steps=4, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model as a part of fitting process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "validation_data=(valid_ds)\n",
    "\n",
    "# Visualize loss history\n",
    "# Get training and test loss histories\n",
    "    \n",
    "training_acc = history.history[\"categorical_accuracy\"]\n",
    "test_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "\n",
    "epoch_count = range(1, len(training_acc) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "\n",
    "plt.plot(epoch_count, training_acc, 'r--')\n",
    "plt.plot(epoch_count, test_acc, 'b-')\n",
    "plt.legend(['Training acc', 'Valid acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)    \n",
    "\n",
    "show_image('download.jpeg')\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    # Print image's original shape, for reference\n",
    "    print('Original image shape: ', mpimg.imread(image_path).shape)\n",
    "    \n",
    "    # Load in the image with a target size of 224, 224\n",
    "    \n",
    "    image = image_utils.load_img(image_path, target_size=(224, 224))\n",
    "    \n",
    "    # Convert the image from a PIL format to a numpy array\n",
    "    image = image_utils.img_to_array(image)\n",
    "    # Add a dimension for number of images, in our case 1\n",
    "    image = image.reshape(1,224,224,3)\n",
    "    \n",
    "    # Preprocess image to align with original ImageNet dataset\n",
    "    \n",
    "    image = preprocess_input(image)\n",
    "    # Print image's shape after processing\n",
    "    print('Processed image shape: ', image.shape)\n",
    "    return image\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "\n",
    "def voice(image_path):\n",
    "    show_image(image_path)\n",
    "    image = load_and_process_image(image_path)\n",
    "    return model.predict(image)\n",
    "\n",
    "a = voice('download.jpeg')\n",
    "\n",
    "if 151 <= np.argmax(a) <= 268:\n",
    "    print(\"Dog!\")\n",
    "    Audio('sound/dog.mp3')\n",
    "elif 281 <= np.argmax(a) <= 285:\n",
    "    print(\"Cat!\")\n",
    "    Audio('sound/cat.mp3')\n",
    "elif 7 <= np.argmax(a) <= 8:\n",
    "    print(\"Chicken!\")\n",
    "    Audio('sound/chicken.mp3')\n",
    "elif 345 <= np.argmax(a) <= 347:\n",
    "    print(\"Cow!\")\n",
    "    Audio('sound/cow.mp3')\n",
    "elif 101 == np.argmax(a) :\n",
    "    print(\"Elephant!\")\n",
    "    Audio('sound/elephant.mp3')\n",
    "elif 385 <= np.argmax(a) <= 386 :\n",
    "    print(\"Elephant!\")\n",
    "    Audio('sound/elephant.mp3')\n",
    "elif 339 == np.argmax(a):\n",
    "    print(\"Horse!\")\n",
    "    Audio('sound/horse.mp3')\n",
    "elif 348 <= np.argmax(a) <= 353 :\n",
    "    print(\"Sheep!\")\n",
    "    Audio('sound/sheep.mp3')\n",
    "elif 335  == np.argmax(a) :\n",
    "    print(\"Squirrel!\")\n",
    "    Audio('sound/squirrel.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(a)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU\n",
    "\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
